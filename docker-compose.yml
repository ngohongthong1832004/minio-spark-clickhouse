# version: '3.7'

# services:
#   clickhouse:
#     image: clickhouse/clickhouse-server:latest
#     container_name: clickhouse
#     ports:
#       - "8123:8123"
#       - "9000:9000"
#     environment:
#       - CLICKHOUSE_USER=default
#       - CLICKHOUSE_PASSWORD=thong123
#     volumes:
#       - clickhouse_data:/var/lib/clickhouse

#   minio:
#     image: minio/minio:latest
#     container_name: minio
#     ports:
#       - "9001:9001"
#       - "9002:9000"
#     environment:
#       - MINIO_ROOT_USER=minioadmin
#       - MINIO_ROOT_PASSWORD=minioadmin123
#     volumes:
#       - ./minio_data:/data
#     command: server /data --console-address ":9001"

#   spark:
#     build:
#       context: ./spark
#       dockerfile: Dockerfile
#     container_name: spark
#     environment:
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin123
#       - SPARK_MODE=driver
#       - SPARK_DRIVER_MEMORY=2g  # Set driver memory
#     volumes:
#       - ./spark:/app
#       - ./spark/app.py:/app/app.py
#       - ./spark/jars:/app/jars  # ✅ Mount thư viện ClickHouse JDBC
#     command: >
#       spark-submit
#       --master local[*]
#       --jars /app/jars/clickhouse-jdbc-all.jar
#       --driver-class-path /app/jars/clickhouse-jdbc-all.jar
#       /app/app.py
# volumes:
#   clickhouse_data:


version: '3.7'

services:
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=thong123
    volumes:
      - clickhouse_data:/var/lib/clickhouse

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9001:9001"
      - "9002:9000"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
    volumes:
      - ./minio_data:/data
    command: server /data --console-address ":9001"

  spark:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark
    hostname: spark
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin123
      - SPARK_MODE=driver
      - SPARK_DRIVER_MEMORY=2g
    volumes:
      - ./spark:/app
      - ./spark/jars:/app/jars
    command: sleep infinity  # để container sống, Airflow sẽ gọi spark-submit từ đây

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow-init:
    image: apache/airflow:2.7.2
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: db init

  airflow-webserver:
    image: apache/airflow:2.7.2
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_WWW_USER_CREATE=True
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.7.2
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: scheduler

volumes:
  clickhouse_data:
  postgres_data:

